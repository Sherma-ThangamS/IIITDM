{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVD18cUbgrFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0646219-c97e-44b2-e48c-3f48e0a99665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade --quiet  ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkJ0GIWfphqH",
        "outputId": "a68d6dfc-edf2-40d2-bf52-351355575514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain-google-genai\n",
            "Version: 0.0.11\n",
            "Summary: An integration package connecting Google's genai package and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain-google\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: google-generativeai, langchain-core\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "# !pip show langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rKDXPESimGvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bedb8e69-f656-48cd-e3f8-bc1e601fe84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain -q\n",
        "# !pip install pinecone-client -q\n",
        "# !pip install pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3E-HOd4osx6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0bf3690-cbee-436b-a6e7-3268aeb24112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GdgT-0tguNND"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet  gpt4all > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZGY5QOJUfaNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba590e9-9a53-4ab2-a548-15a4b90ead03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b4V2rnsln538"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.vectorstores import Pinecone ,FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "# import pinecone\n",
        "import os\n",
        "import sys\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "# from langchain_mistralai import MistralAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI ,GoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U_EWrHzisuF",
        "outputId": "b5a1686d-d6ea-442e-a7c7-981a0251635f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btViLRdvpht6"
      },
      "source": [
        "#**Step 03: Load the PDF Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrXjdVWPpY6I"
      },
      "outputs": [],
      "source": [
        "# !mkdir pdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSnI--9TpmZl"
      },
      "outputs": [],
      "source": [
        "# !gdown 1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE -O pdfs/yolov7paper.pdf\n",
        "# !gdown 1vILwiv6nS2wI3chxNabMgry3qnV67TxM -O pdfs/rachelgreecv.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxKDyZwUpuwI"
      },
      "source": [
        "#**Step 04: Extract the Text from the PDF's**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p9KPyfPpoGA"
      },
      "outputs": [],
      "source": [
        "# loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "# data = loader.load()\n",
        "loader=TextLoader(\"/content/22EC402 LIC UNIT IV.pdf\")\n",
        "data=loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqMAX9i1qCtP"
      },
      "outputs": [],
      "source": [
        "# print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhAHycYuqRDB"
      },
      "source": [
        "#**Step 05: Split the Extracted Data into Text Chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PObfNUEcqGJe"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE5gbklRqg-X"
      },
      "outputs": [],
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVq2Eh_NjSBy"
      },
      "outputs": [],
      "source": [
        "# len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40lbTG73qlWK"
      },
      "outputs": [],
      "source": [
        "# text_chunks[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuQE-Eh2qqjo"
      },
      "outputs": [],
      "source": [
        "# text_chunks[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCZSH6OYsW6A"
      },
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyAANEPA1UF6WE4O_0GQh2s27iBT4VrN0Ag'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFWmIx64s6JK"
      },
      "source": [
        "#**Step 06:Downlaod the Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuTrw7qqY8c6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGOBwYepqxZk",
        "outputId": "2ae7564e-9070-4d4f-b65b-193ec20b016e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45.9M/45.9M [00:00<00:00, 156MiB/s]\n"
          ]
        }
      ],
      "source": [
        "embeddings=GPT4AllEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqFDvIFvrD3_"
      },
      "outputs": [],
      "source": [
        "# query_result = embeddings.embed_query(\"Hello World\")\n",
        "# #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPiI4LUJtApc"
      },
      "outputs": [],
      "source": [
        "# print(\"Length\", len(query_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFPWq-JwtLml"
      },
      "source": [
        "#**Step 07: Initializing the Pinecone**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46k7HddwtG9S"
      },
      "outputs": [],
      "source": [
        "# PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '3079e43b-eda3-4433-ba60-2340a128edfa')\n",
        "# PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'asia-southeast1-gcp-free')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrgW6s18uKOi"
      },
      "outputs": [],
      "source": [
        "# # initialize pinecone\n",
        "# pinecone.init(\n",
        "#     api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "#     environment=PINECONE_API_ENV  # next to api key in console\n",
        "# )\n",
        "# index_name = \"langchainpinecone\" # put in the name of your pinecone index here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KLS5dhHuOYk"
      },
      "source": [
        "#**Step 08: Create Embeddings for each of the Text Chunk**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLhm8k-vuMQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "cd29a42b-27f2-463a-c69b-c11271fa3929"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text_chunks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5762d1f1ae7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text_chunks' is not defined"
          ]
        }
      ],
      "source": [
        "db = FAISS.from_documents(text_chunks, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvMLt2poe2E2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "0b6e0919-d3f5-4d23-9b8b-0e0349d1f9d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'db' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-50b54d216f06>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/FAISS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
          ]
        }
      ],
      "source": [
        "db.save_local(\"/content/FAISS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQFNB1_txkz4"
      },
      "source": [
        "#**Step 09: Similarity Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aIlXvCqmNfi"
      },
      "outputs": [],
      "source": [
        "db=FAISS.load_local(\"/content/drive/MyDrive/GeneralRMK\",embeddings,allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Q9eQzCxtx4"
      },
      "outputs": [],
      "source": [
        "query = \"Why RMK is the Best college?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUQt8hsSxofW"
      },
      "outputs": [],
      "source": [
        "docs = db.similarity_search(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKfU_dC0xr6_",
        "outputId": "2aaf80e8-d9a6-42cd-f869-c7d08dba9168"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='of character and discipline are imparted to students to become good citizens capable of utilizing their talents for the development of our Nation.Chairman’s MessageDear Students,Welcome to RMK Engineering College!The transition from school to college is a very big step in life. You have put in so much hard work in your public exams, spent sleepless nights, earned a rewarding score, and with the suggestions of your parents/guardian you have joined your chosen stream of education to realize your', metadata={'source': '/content/output.txt'}),\n",
              " Document(page_content='of character and discipline are imparted to students to become good citizens capable of utilizing their talents for the development of our Nation.Chairman’s MessageDear Students,Welcome to RMK Engineering College!The transition from school to college is a very big step in life. You have put in so much hard work in your public exams, spent sleepless nights, earned a rewarding score, and with the suggestions of your parents/guardian you have joined your chosen stream of education to realize your', metadata={'source': '/content/output.txt'}),\n",
              " Document(page_content='of character and discipline are imparted to students to become good citizens capable of utilizing their talents for the development of our Nation.Chairman’s MessageDear Students,Welcome to RMK Engineering College!The transition from school to college is a very big step in life. You have put in so much hard work in your public exams, spent sleepless nights, earned a rewarding score, and with the suggestions of your parents/guardian you have joined your chosen stream of education to realize your', metadata={'source': '/content/output.txt'}),\n",
              " Document(page_content='RMK offers an ever adapting and dynamic learning process across all its institutions.  A highly qualified faculty, across disciplines\\nUniform class size and student-to-faculty ratio', metadata={'source': '/content/output.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1DZj6HVx3iv"
      },
      "source": [
        "#**Step 10: Creating a Google PaLM Model Wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Raf4vib9xsZW"
      },
      "outputs": [],
      "source": [
        "llm = GoogleGenerativeAI(model=\"models/text-bison-001\",convert_system_message_to_human=True,verbose=True,google_api_key=\"AIzaSyAtp_lUKFAXhp9O1B_nmg_pvWGAuVxaXZ8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro-YK_cJgwgb"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.llms import CTransformers\n",
        "\n",
        "# llm = CTransformers(model=\"TheBloke/Llama-2-7B-Chat-GGML\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "0ekoHmMJrQon",
        "outputId": "7841ea15-f6d4-4727-e7c6-b7a53706dfba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"’s 65% Ergonomic Keyboard is the Best for Programmers There are many different types of keyboards on the market, but not all of them are created equal. If you're a programmer, you need a keyboard that is specifically designed for your needs. The 65% Ergonomic Keyboard from Whynter is the best option for programmers because it offers a number of features that make it ideal for this type of work.\\n\\n**Comfortable Design**\\n\\nOne of the most important factors to consider when choosing a keyboard is comfort. The 65% Ergonomic Keyboard from Whynter is designed to be as comfortable as possible to use, even for long periods of time. The keys are spaced out evenly, and the wrist rest helps to keep your wrists in a neutral position.\\n\\n**Ergonomic Layout**\\n\\nThe 65% Ergonomic Keyboard from Whynter is also designed with ergonomics in mind. The keys are arranged in a staggered layout, which helps to reduce strain on your wrists and fingers. The keyboard also has a low profile, which makes it easier to type on.\\n\\n**Detachable Cable**\\n\\nThe 65% Ergonomic Keyboard from Whynter has a detachable cable, which makes it easy to take with you wherever you go. This is a great feature for programmers who work on multiple computers or who need to travel frequently.\\n\\n**Backlit Keys**\\n\\nThe 65% Ergonomic Keyboard from Whynter also has backlit keys, which makes it easy to type in low-light conditions. This is a great feature for programmers who work late into the night or who need to work in dimly lit environments.\\n\\n**Reprogrammable Keys**\\n\\nThe 65% Ergonomic Keyboard from Whynter has reprogrammable keys, which gives you the ability to customize the keyboard to your own needs. This is a great feature for programmers who need to have specific keys for specific functions.\\n\\n**Overall, the 65% Ergonomic Keyboard from Whynter is the best option for programmers because it offers a number of features that make it ideal for this type of work. It is comfortable, ergonomically designed, has a detachable cable, backlit keys, and reprogrammable keys.**\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "llm.invoke(\"Why RMK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFZOSE4XyFlK"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever(k=5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp2IlRIJ3FBt"
      },
      "source": [
        "#Custom Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkLUm1jZ3Bzg"
      },
      "outputs": [],
      "source": [
        "prompt_template  = \"\"\"\n",
        " Use the following piece of context to answer the question. Use MUST provide a very detailed Answer atleast for each the question.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# You are a Chatbot Assisstant that belongs to the R.M.K Engineering College .  You should help the public by providing accurate information about the R.M.K engineering College ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyty8Sbt4iAS"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template = prompt_template , input_variables=[\"context\", \"question\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp-cFGM224lC"
      },
      "source": [
        "#**Step 11: Q/A**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6GtUpQyzkmf"
      },
      "outputs": [],
      "source": [
        "query = \"Why RMK is the Best College?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "c6hp6fDz0uvo",
        "outputId": "d6335029-fea2-4df9-c060-38afc2388213"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'qa' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f6d3efa29314>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'qa' is not defined"
          ]
        }
      ],
      "source": [
        "qa.invoke(query)['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-EG-EYa0v2J"
      },
      "outputs": [],
      "source": [
        "query = \"Rachel Green Experience\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "fkwxv_IR018W",
        "outputId": "13f1d314-4001-46ad-c280-136f1939e5c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"don't know\""
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "igXUEvnX9UUI",
        "outputId": "bcde425b-e3f7-4284-dc7e-8cfa28994edf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Yes, to practise any profession, or to carry on any occupation, trade or business.\n",
            "Answer: no\n",
            "Answer: no\n",
            "Answer: no\n",
            "Answer: no, it is not allowed.\n",
            "Answer: Section 41 of the Penal Code, 1860 states that \"Nothing is an offence which is done in the exercise of the right of private defence.\"\n",
            "Answer: 378\n",
            "Answer: Section 378 of the Constitution of India states that no law made by the Parliament or by the Legislature of a State shall be invalid on the ground that it is inconsistent with, or takes away or abridges any of the rights conferred by, any provisions of this part, and notwithstanding any judgment, decree or order of any court or tribunal to the contrary.\n",
            "Answer: don't know\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp5wNXhE7M7_"
      },
      "source": [
        "#**Step 12: Q/ A with Custom Prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SikJJl5o1zB6"
      },
      "outputs": [],
      "source": [
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever(k=5), chain_type_kwargs=chain_type_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9ku9vbd7ZYT"
      },
      "outputs": [],
      "source": [
        "query = \"Why RMK is the Best College\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vluo_a7a7bYP",
        "outputId": "acd5d7aa-5cae-4c14-a2d7-af4e50bd4ada"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Why RMK is the Best College',\n",
              " 'result': 'There are many reasons why RMK Engineering College is considered to be one of the best colleges in India. \\n\\n* **RMK offers an ever adapting and dynamic learning process across all its institutions.** The college has a strong focus on research and innovation, and it encourages students to think outside the box and come up with new ideas. The college also offers a variety of extracurricular activities, which help students to develop their leadership, teamwork, and communication skills.\\n* **A highly qualified faculty, across disciplines**. The faculty at RMK are experts in their field, and they are passionate about teaching. They are committed to helping students succeed, and they are always available to answer questions and provide guidance.\\n* **Uniform class size and student-to-faculty ratio**. The class sizes at RMK are small, which allows for more personalized attention from the faculty. This helps students to get the most out of their education.\\n* **Well-equipped labs and facilities**. RMK has well-equipped labs and facilities, which give students the opportunity to learn in a practical setting. The college also has a strong research culture, which encourages students to get involved in research projects.\\n* **Placements**. RMK has a strong track record of placements. The college has a dedicated placement cell that helps students to find jobs after graduation. The cell works with top companies to ensure that students have the opportunity to get their dream jobs.\\n\\nOverall, RMK Engineering College is a great option for students who are looking for a challenging and rewarding college experience. The college offers a high-quality education, and it has a strong focus on research and innovation. The faculty are experts in their field, and they are committed to helping students succeed. The college also has a strong placement record, which ensures that students have the opportunity to get their dream jobs after graduation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "qa.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS2NSJBknn01"
      },
      "outputs": [],
      "source": [
        "def ask(question):\n",
        "  return qa.invoke(question)['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccKQBGUWnvhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2deb470-0cf9-44e5-d6a5-c438812f723c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio -U -q\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VRvolc3j7gxv",
        "outputId": "4d376800-43b8-44fe-c90e-9fabcbafa573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://d59373b6e914c6ec27.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d59373b6e914c6ec27.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 72, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1176, in __call__\n",
            "    return _end_unary_response_blocking(state, call, False, None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1005, in _end_unary_response_blocking\n",
            "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
            "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.FAILED_PRECONDITION\n",
            "\tdetails = \"User location is not supported for the API use.\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.128.95:443 {grpc_message:\"User location is not supported for the API use.\", grpc_status:9, created_time:\"2024-03-15T05:18:20.565824911+00:00\"}\"\n",
            ">\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 90, in _completion_with_retry\n",
            "    return llm.client.generate_text(prompt=prompt, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 202, in generate_text\n",
            "    return _generate_response(client=client, request=request, request_options=request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 240, in _generate_response\n",
            "    response = client.generate_text(request, **request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py\", line 648, in generate_text\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 113, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 349, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 191, in retry_target\n",
            "    return target()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\", line 120, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 74, in error_remapped_callable\n",
            "    raise exceptions.from_grpc_error(exc) from exc\n",
            "google.api_core.exceptions.FailedPrecondition: 400 User location is not supported for the API use.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 253, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1695, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1235, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 692, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-17-ca0be34c2c2c>\", line 2, in ask\n",
            "    return qa.invoke(question)['result']\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 163, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 153, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\", line 144, in _call\n",
            "    answer = self.combine_documents_chain.run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 550, in run\n",
            "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 378, in __call__\n",
            "    return self.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 163, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 153, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\n",
            "    output, extra_return_dict = self.combine_docs(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\n",
            "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 293, in predict\n",
            "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 378, in __call__\n",
            "    return self.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 163, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 153, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 103, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 115, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 541, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 714, in generate\n",
            "    output = self._generate_helper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 578, in _generate_helper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 565, in _generate_helper\n",
            "    self._generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 280, in _generate\n",
            "    res = _completion_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 95, in _completion_with_retry\n",
            "    return _completion_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
            "    return self(f, *args, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 314, in iter\n",
            "    return fut.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 93, in _completion_with_retry\n",
            "    raise ValueError(error_msg)\n",
            "ValueError: Your location is not supported by google-generativeai at the moment. Try to use VertexAI LLM from langchain_google_vertexai\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 72, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1176, in __call__\n",
            "    return _end_unary_response_blocking(state, call, False, None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\", line 1005, in _end_unary_response_blocking\n",
            "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
            "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.FAILED_PRECONDITION\n",
            "\tdetails = \"User location is not supported for the API use.\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.128.95:443 {created_time:\"2024-03-15T05:18:35.955382269+00:00\", grpc_status:9, grpc_message:\"User location is not supported for the API use.\"}\"\n",
            ">\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 90, in _completion_with_retry\n",
            "    return llm.client.generate_text(prompt=prompt, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 202, in generate_text\n",
            "    return _generate_response(client=client, request=request, request_options=request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 240, in _generate_response\n",
            "    response = client.generate_text(request, **request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py\", line 648, in generate_text\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 113, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 349, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 191, in retry_target\n",
            "    return target()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\", line 120, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 74, in error_remapped_callable\n",
            "    raise exceptions.from_grpc_error(exc) from exc\n",
            "google.api_core.exceptions.FailedPrecondition: 400 User location is not supported for the API use.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 253, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1695, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1235, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 692, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-17-ca0be34c2c2c>\", line 2, in ask\n",
            "    return qa.invoke(question)['result']\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 163, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 153, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\", line 144, in _call\n",
            "    answer = self.combine_documents_chain.run(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 550, in run\n",
            "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 378, in __call__\n",
            "    return self.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 163, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 153, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\n",
            "    output, extra_return_dict = self.combine_docs(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\n",
            "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 293, in predict\n",
            "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 378, in __call__\n",
            "    return self.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 163, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 153, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 103, in _call\n",
            "    response = self.generate([inputs], run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\", line 115, in generate\n",
            "    return self.llm.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 541, in generate_prompt\n",
            "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 714, in generate\n",
            "    output = self._generate_helper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 578, in _generate_helper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\", line 565, in _generate_helper\n",
            "    self._generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 280, in _generate\n",
            "    res = _completion_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 95, in _completion_with_retry\n",
            "    return _completion_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
            "    return self(f, *args, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 379, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 314, in iter\n",
            "    return fut.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 382, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_google_genai/llms.py\", line 93, in _completion_with_retry\n",
            "    raise ValueError(error_msg)\n",
            "ValueError: Your location is not supported by google-generativeai at the moment. Try to use VertexAI LLM from langchain_google_vertexai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d59373b6e914c6ec27.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "iface = gr.Interface(fn=ask, inputs=gr.Textbox(\n",
        "    value=\"Why should you choose RMK over other colleges\"),\n",
        "        outputs=\"markdown\",\n",
        "        title=\"Test Site\",\n",
        "        description=\"Ask a question our RMK college.\",\n",
        "        examples=[['What are the departments are there in RMK'],['What is so special about E.C.E'],['Why R.M.K is the best College'],['Tell me more about the placement in RMK']],\n",
        "    theme=gr.themes.Soft(),\n",
        "    allow_flagging=\"never\",)\n",
        "\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRP8zFWOo8U6",
        "outputId": "188aaa58-5078-4ea0-b42f-d33463e06478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Need \u001b[32m'write'\u001b[0m access token to create a Spaces repo.\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Creating new Spaces Repo in \u001b[32m'/content'\u001b[0m. Collecting metadata, press Enter to accept default value.\n",
            "Enter Spaces app title [content]: RMKchatbot\n",
            "Enter Gradio app file : \n",
            "\u001b[31mAborted.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!gradio deploy"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}